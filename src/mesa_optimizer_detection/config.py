"""
Configuration Management for Mesa-Optimizer Detection

This module provides configuration classes and validation for all detection methods.
All configurations include proper validation and safe defaults.

IMPORTANT: This framework is for research purposes only. Mesa-optimization detection
is an unsolved problem in AI safety. Do not rely on these results for critical
safety decisions without additional validation.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, List
import logging
import warnings
import numpy as np

logger = logging.getLogger(__name__)

# Issue a warning about the experimental nature of this framework
warnings.warn(
    "Mesa-Optimizer Detection Framework is experimental and should not be used "
    "for critical safety decisions without additional validation. False positives "
    "and false negatives are likely.", 
    UserWarning,
    stacklevel=2
)


@dataclass 
class RiskThresholds:
    """Thresholds for categorizing risk levels."""
    low: float = 0.4  # Increased from 0.3 to reduce false positives
    medium: float = 0.65  # Increased from 0.6
    high: float = 0.85  # Increased from 0.8
    
    def __post_init__(self):
        """Validate thresholds after initialization."""
        self._validate_thresholds()
    
    def _validate_thresholds(self):
        """Validate that thresholds are properly ordered and in valid range."""
        try:
            # Validate types and convert to float
            self.low = float(self.low) if self.low is not None else 0.4
            self.medium = float(self.medium) if self.medium is not None else 0.65
            self.high = float(self.high) if self.high is not None else 0.85
            
            # Check for NaN/Inf values
            if any(np.isnan(x) or np.isinf(x) for x in [self.low, self.medium, self.high]):
                raise ValueError("Threshold values cannot be NaN or Inf")
            
            # Clamp values to valid range [0, 1]
            self.low = max(0.0, min(1.0, self.low))
            self.medium = max(0.0, min(1.0, self.medium))
            self.high = max(0.0, min(1.0, self.high))
            
            # Ensure proper ordering
            if not (0.0 <= self.low <= self.medium <= self.high <= 1.0):
                logger.warning("Risk thresholds not properly ordered, using defaults")
                self.low = 0.4
                self.medium = 0.65
                self.high = 0.85
        except (ValueError, TypeError) as e:
            logger.warning(f"Threshold validation failed: {e}, using defaults")
            self.low = 0.4
            self.medium = 0.65
            self.high = 0.85


@dataclass
class GradientConfig:
    """Configuration for gradient-based detection."""
    variance_threshold: float = 0.7  # Increased from 0.5 to reduce false positives
    anomaly_threshold: float = 0.8  # Increased from 0.7
    hessian_analysis: bool = False
    max_eigenvalues: int = 10
    
    # New parameters for robustness
    min_samples_for_analysis: int = 5  # Minimum samples needed for reliable analysis
    outlier_rejection_threshold: float = 3.0  # Standard deviations for outlier rejection
    smoothing_factor: float = 0.3  # Exponential smoothing for gradient history
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate gradient configuration parameters."""
        # Validate variance threshold
        if not isinstance(self.variance_threshold, (int, float)) or self.variance_threshold < 0:
            logger.warning(f"Invalid variance_threshold: {self.variance_threshold}, using 0.7")
            self.variance_threshold = 0.7
        self.variance_threshold = max(0.0, min(10.0, float(self.variance_threshold)))
        
        # Validate anomaly threshold
        if not isinstance(self.anomaly_threshold, (int, float)):
            logger.warning(f"Invalid anomaly_threshold: {self.anomaly_threshold}, using 0.8")
            self.anomaly_threshold = 0.8
        self.anomaly_threshold = max(0.0, min(1.0, float(self.anomaly_threshold)))
        
        # Validate hessian analysis flag
        if not isinstance(self.hessian_analysis, bool):
            logger.warning(f"Invalid hessian_analysis: {self.hessian_analysis}, using False")
            self.hessian_analysis = False
        
        # Validate max eigenvalues
        if not isinstance(self.max_eigenvalues, int) or self.max_eigenvalues <= 0:
            logger.warning(f"Invalid max_eigenvalues: {self.max_eigenvalues}, using 10")
            self.max_eigenvalues = 10
        self.max_eigenvalues = max(1, min(50, self.max_eigenvalues))
        
        # Validate new parameters
        if not isinstance(self.min_samples_for_analysis, int) or self.min_samples_for_analysis <= 0:
            logger.warning(f"Invalid min_samples_for_analysis: {self.min_samples_for_analysis}, using 5")
            self.min_samples_for_analysis = 5
        self.min_samples_for_analysis = max(1, min(100, self.min_samples_for_analysis))
        
        if not isinstance(self.outlier_rejection_threshold, (int, float)) or self.outlier_rejection_threshold <= 0:
            logger.warning(f"Invalid outlier_rejection_threshold: {self.outlier_rejection_threshold}, using 3.0")
            self.outlier_rejection_threshold = 3.0
        self.outlier_rejection_threshold = max(1.0, min(10.0, float(self.outlier_rejection_threshold)))
        
        if not isinstance(self.smoothing_factor, (int, float)) or not 0 <= self.smoothing_factor <= 1:
            logger.warning(f"Invalid smoothing_factor: {self.smoothing_factor}, using 0.3")
            self.smoothing_factor = 0.3
        self.smoothing_factor = max(0.0, min(1.0, float(self.smoothing_factor)))


@dataclass
class ActivationConfig:
    """Configuration for activation pattern analysis."""
    entropy_threshold: float = 0.85  # Increased from 0.8
    sparsity_threshold: float = 0.95  # Increased from 0.9
    correlation_threshold: float = 0.8  # Increased from 0.7
    min_activation_samples: int = 200  # Increased from 100
    
    # New parameters for robustness
    pattern_consistency_threshold: float = 0.8  # Patterns must be consistent
    statistical_significance_threshold: float = 0.05  # P-value threshold
    cross_validation_folds: int = 5  # Cross-validation for pattern detection
    
    # Thresholds for higher-level risk components
    planning_threshold: float = 0.3  # default for planning score risk
    goal_threshold: float = 0.3      # default for goal-directedness risk
    optimization_threshold: float = 0.3  # default for optimization circuit risk
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate activation configuration parameters."""
        # Validate entropy threshold
        if not isinstance(self.entropy_threshold, (int, float)):
            logger.warning(f"Invalid entropy_threshold: {self.entropy_threshold}, using 0.85")
            self.entropy_threshold = 0.85
        self.entropy_threshold = max(0.0, min(1.0, float(self.entropy_threshold)))
        
        # Validate sparsity threshold
        if not isinstance(self.sparsity_threshold, (int, float)):
            logger.warning(f"Invalid sparsity_threshold: {self.sparsity_threshold}, using 0.95")
            self.sparsity_threshold = 0.95
        self.sparsity_threshold = max(0.0, min(1.0, float(self.sparsity_threshold)))
        
        # Validate correlation threshold
        if not isinstance(self.correlation_threshold, (int, float)):
            logger.warning(f"Invalid correlation_threshold: {self.correlation_threshold}, using 0.8")
            self.correlation_threshold = 0.8
        self.correlation_threshold = max(0.0, min(1.0, float(self.correlation_threshold)))
        
        # Validate minimum samples
        if not isinstance(self.min_activation_samples, int) or self.min_activation_samples <= 0:
            logger.warning(f"Invalid min_activation_samples: {self.min_activation_samples}, using 200")
            self.min_activation_samples = 200
        self.min_activation_samples = max(1, min(10000, self.min_activation_samples))
        
        # Validate new parameters
        if not isinstance(self.pattern_consistency_threshold, (int, float)):
            logger.warning(f"Invalid pattern_consistency_threshold: {self.pattern_consistency_threshold}, using 0.8")
            self.pattern_consistency_threshold = 0.8
        self.pattern_consistency_threshold = max(0.0, min(1.0, float(self.pattern_consistency_threshold)))
        
        if not isinstance(self.statistical_significance_threshold, (int, float)):
            logger.warning(f"Invalid statistical_significance_threshold: {self.statistical_significance_threshold}, using 0.05")
            self.statistical_significance_threshold = 0.05
        self.statistical_significance_threshold = max(0.001, min(0.5, float(self.statistical_significance_threshold)))
        
        if not isinstance(self.cross_validation_folds, int) or self.cross_validation_folds <= 0:
            logger.warning(f"Invalid cross_validation_folds: {self.cross_validation_folds}, using 5")
            self.cross_validation_folds = 5
        self.cross_validation_folds = max(2, min(20, self.cross_validation_folds))
        
        # Validate new high-level thresholds
        for name in ("planning_threshold", "goal_threshold", "optimization_threshold"):
            val = getattr(self, name, 0.3)
            if not isinstance(val, (int, float)):
                logger.warning(f"Invalid {name}: {val}, using 0.3")
                setattr(self, name, 0.3)
            else:
                setattr(self, name, max(0.0, min(1.0, float(val))))


@dataclass
class BehavioralConfig:
    """Configuration for behavioral consistency analysis."""
    consistency_threshold: float = 0.8  # Increased from 0.7
    context_sensitivity_threshold: float = 0.6  # Increased from 0.5
    max_response_length: int = 512
    num_test_contexts: int = 8  # Increased from 5
    
    # New parameters for robustness
    min_context_diversity: float = 0.5  # Minimum diversity between test contexts
    response_stability_threshold: float = 0.7  # Threshold for response stability
    baseline_comparison_required: bool = True  # Require baseline comparison
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate behavioral configuration parameters."""
        # Validate consistency threshold
        if not isinstance(self.consistency_threshold, (int, float)):
            logger.warning(f"Invalid consistency_threshold: {self.consistency_threshold}, using 0.8")
            self.consistency_threshold = 0.8
        self.consistency_threshold = max(0.0, min(1.0, float(self.consistency_threshold)))
        
        # Validate context sensitivity threshold
        if not isinstance(self.context_sensitivity_threshold, (int, float)):
            logger.warning(f"Invalid context_sensitivity_threshold: {self.context_sensitivity_threshold}, using 0.6")
            self.context_sensitivity_threshold = 0.6
        self.context_sensitivity_threshold = max(0.0, min(1.0, float(self.context_sensitivity_threshold)))
        
        # Validate max response length
        if not isinstance(self.max_response_length, int) or self.max_response_length <= 0:
            logger.warning(f"Invalid max_response_length: {self.max_response_length}, using 512")
            self.max_response_length = 512
        self.max_response_length = max(1, min(4096, self.max_response_length))
        
        # Validate number of test contexts
        if not isinstance(self.num_test_contexts, int) or self.num_test_contexts <= 0:
            logger.warning(f"Invalid num_test_contexts: {self.num_test_contexts}, using 8")
            self.num_test_contexts = 8
        self.num_test_contexts = max(1, min(20, self.num_test_contexts))
        
        # Validate new parameters
        if not isinstance(self.min_context_diversity, (int, float)):
            logger.warning(f"Invalid min_context_diversity: {self.min_context_diversity}, using 0.5")
            self.min_context_diversity = 0.5
        self.min_context_diversity = max(0.0, min(1.0, float(self.min_context_diversity)))
        
        if not isinstance(self.response_stability_threshold, (int, float)):
            logger.warning(f"Invalid response_stability_threshold: {self.response_stability_threshold}, using 0.7")
            self.response_stability_threshold = 0.7
        self.response_stability_threshold = max(0.0, min(1.0, float(self.response_stability_threshold)))
        
        if not isinstance(self.baseline_comparison_required, bool):
            logger.warning(f"Invalid baseline_comparison_required: {self.baseline_comparison_required}, using True")
            self.baseline_comparison_required = True


@dataclass
class DynamicsConfig:
    """Configuration for optimization dynamics monitoring."""
    learning_rate_threshold: float = 0.15  # Increased from 0.1
    loss_variance_threshold: float = 0.7  # Increased from 0.5
    gradient_norm_threshold: float = 1.5  # Increased from 1.0
    window_size: int = 150  # Increased from 100
    history_window: int = 1000  # Size of loss history window
    
    # New parameters for robustness
    phase_transition_confidence: float = 0.8  # Confidence threshold for phase detection
    trend_significance_threshold: float = 0.05  # Statistical significance for trends
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate dynamics configuration parameters."""
        # Validate learning rate threshold
        if not isinstance(self.learning_rate_threshold, (int, float)) or self.learning_rate_threshold < 0:
            logger.warning(f"Invalid learning_rate_threshold: {self.learning_rate_threshold}, using 0.15")
            self.learning_rate_threshold = 0.15
        self.learning_rate_threshold = max(0.0, min(1.0, float(self.learning_rate_threshold)))
        
        # Validate loss variance threshold
        if not isinstance(self.loss_variance_threshold, (int, float)) or self.loss_variance_threshold < 0:
            logger.warning(f"Invalid loss_variance_threshold: {self.loss_variance_threshold}, using 0.7")
            self.loss_variance_threshold = 0.7
        self.loss_variance_threshold = max(0.0, min(10.0, float(self.loss_variance_threshold)))
        
        # Validate gradient norm threshold
        if not isinstance(self.gradient_norm_threshold, (int, float)) or self.gradient_norm_threshold < 0:
            logger.warning(f"Invalid gradient_norm_threshold: {self.gradient_norm_threshold}, using 1.5")
            self.gradient_norm_threshold = 1.5
        self.gradient_norm_threshold = max(0.0, min(100.0, float(self.gradient_norm_threshold)))
        
        # Validate window size
        if not isinstance(self.window_size, int) or self.window_size <= 0:
            logger.warning(f"Invalid window_size: {self.window_size}, using 150")
            self.window_size = 150
        self.window_size = max(1, min(10000, self.window_size))
        
        # Validate new parameters
        if not isinstance(self.phase_transition_confidence, (int, float)):
            logger.warning(f"Invalid phase_transition_confidence: {self.phase_transition_confidence}, using 0.8")
            self.phase_transition_confidence = 0.8
        self.phase_transition_confidence = max(0.0, min(1.0, float(self.phase_transition_confidence)))
        
        if not isinstance(self.trend_significance_threshold, (int, float)):
            logger.warning(f"Invalid trend_significance_threshold: {self.trend_significance_threshold}, using 0.05")
            self.trend_significance_threshold = 0.05
        self.trend_significance_threshold = max(0.001, min(0.5, float(self.trend_significance_threshold)))


@dataclass
class CausalConfig:
    """Configuration for causal intervention analysis."""
    intervention_strength: float = 0.15  # Increased from 0.1
    num_interventions: int = 15  # Increased from 10
    effect_threshold: float = 0.4  # Increased from 0.3
    max_layers_to_intervene: int = 5
    
    # New parameters for robustness
    control_group_size: int = 5  # Size of control group for comparison
    statistical_power_threshold: float = 0.8  # Minimum statistical power
    effect_size_threshold: float = 0.3  # Minimum effect size for significance
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate causal configuration parameters."""
        # Validate intervention strength
        if not isinstance(self.intervention_strength, (int, float)) or self.intervention_strength < 0:
            logger.warning(f"Invalid intervention_strength: {self.intervention_strength}, using 0.15")
            self.intervention_strength = 0.15
        self.intervention_strength = max(0.0, min(1.0, float(self.intervention_strength)))
        
        # Validate number of interventions
        if not isinstance(self.num_interventions, int) or self.num_interventions <= 0:
            logger.warning(f"Invalid num_interventions: {self.num_interventions}, using 15")
            self.num_interventions = 15
        self.num_interventions = max(1, min(100, self.num_interventions))
        
        # Validate effect threshold
        if not isinstance(self.effect_threshold, (int, float)):
            logger.warning(f"Invalid effect_threshold: {self.effect_threshold}, using 0.4")
            self.effect_threshold = 0.4
        self.effect_threshold = max(0.0, min(1.0, float(self.effect_threshold)))
        
        # Validate max layers to intervene
        if not isinstance(self.max_layers_to_intervene, int) or self.max_layers_to_intervene <= 0:
            logger.warning(f"Invalid max_layers_to_intervene: {self.max_layers_to_intervene}, using 5")
            self.max_layers_to_intervene = 5
        self.max_layers_to_intervene = max(1, min(20, self.max_layers_to_intervene))
        
        # Validate new parameters
        if not isinstance(self.control_group_size, int) or self.control_group_size <= 0:
            logger.warning(f"Invalid control_group_size: {self.control_group_size}, using 5")
            self.control_group_size = 5
        self.control_group_size = max(1, min(50, self.control_group_size))
        
        if not isinstance(self.statistical_power_threshold, (int, float)):
            logger.warning(f"Invalid statistical_power_threshold: {self.statistical_power_threshold}, using 0.8")
            self.statistical_power_threshold = 0.8
        self.statistical_power_threshold = max(0.0, min(1.0, float(self.statistical_power_threshold)))
        
        if not isinstance(self.effect_size_threshold, (int, float)):
            logger.warning(f"Invalid effect_size_threshold: {self.effect_size_threshold}, using 0.3")
            self.effect_size_threshold = 0.3
        self.effect_size_threshold = max(0.0, min(2.0, float(self.effect_size_threshold)))


@dataclass
class UncertaintyConfig:
    """Configuration for uncertainty quantification in detection results."""
    enable_uncertainty_quantification: bool = True
    monte_carlo_samples: int = 100  # Number of MC samples for uncertainty estimation
    bootstrap_samples: int = 1000  # Bootstrap samples for confidence intervals
    confidence_interval_alpha: float = 0.05  # Alpha level for confidence intervals (95% CI)
    
    # Bayesian parameters
    prior_mesa_probability: float = 0.01  # Prior probability of mesa-optimization
    evidence_weight_decay: float = 0.9  # Decay factor for evidence over time
    
    def __post_init__(self):
        """Validate uncertainty configuration."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate uncertainty configuration parameters."""
        if not isinstance(self.enable_uncertainty_quantification, bool):
            logger.warning("Invalid enable_uncertainty_quantification, using True")
            self.enable_uncertainty_quantification = True
        
        if not isinstance(self.monte_carlo_samples, int) or self.monte_carlo_samples <= 0:
            logger.warning(f"Invalid monte_carlo_samples: {self.monte_carlo_samples}, using 100")
            self.monte_carlo_samples = 100
        self.monte_carlo_samples = max(10, min(10000, self.monte_carlo_samples))
        
        if not isinstance(self.bootstrap_samples, int) or self.bootstrap_samples <= 0:
            logger.warning(f"Invalid bootstrap_samples: {self.bootstrap_samples}, using 1000")
            self.bootstrap_samples = 1000
        self.bootstrap_samples = max(100, min(100000, self.bootstrap_samples))
        
        if not isinstance(self.confidence_interval_alpha, (int, float)) or not 0 < self.confidence_interval_alpha < 1:
            logger.warning(f"Invalid confidence_interval_alpha: {self.confidence_interval_alpha}, using 0.05")
            self.confidence_interval_alpha = 0.05
        self.confidence_interval_alpha = max(0.001, min(0.5, float(self.confidence_interval_alpha)))
        
        if not isinstance(self.prior_mesa_probability, (int, float)) or not 0 < self.prior_mesa_probability < 1:
            logger.warning(f"Invalid prior_mesa_probability: {self.prior_mesa_probability}, using 0.01")
            self.prior_mesa_probability = 0.01
        self.prior_mesa_probability = max(0.001, min(0.5, float(self.prior_mesa_probability)))
        
        if not isinstance(self.evidence_weight_decay, (int, float)) or not 0 < self.evidence_weight_decay <= 1:
            logger.warning(f"Invalid evidence_weight_decay: {self.evidence_weight_decay}, using 0.9")
            self.evidence_weight_decay = 0.9
        self.evidence_weight_decay = max(0.1, min(1.0, float(self.evidence_weight_decay)))


@dataclass
class DetectionConfig:
    """Main configuration class for mesa-optimizer detection."""
    
    # Risk assessment configuration
    risk_thresholds: RiskThresholds = field(default_factory=RiskThresholds)
    
    # Method-specific configurations
    gradient_config: GradientConfig = field(default_factory=GradientConfig)
    activation_config: ActivationConfig = field(default_factory=ActivationConfig)
    behavioral_config: BehavioralConfig = field(default_factory=BehavioralConfig)
    dynamics_config: DynamicsConfig = field(default_factory=DynamicsConfig)
    causal_config: CausalConfig = field(default_factory=CausalConfig)
    uncertainty_config: UncertaintyConfig = field(default_factory=UncertaintyConfig)
    
    # Method weighting for risk aggregation (adjusted for better balance)
    method_weights: Dict[str, float] = field(default_factory=lambda: {
        'gradient': 0.8,      # Reduced weight due to high false positive rate
        'activation': 1.2,    # Increased weight as it's more specific
        'behavioral': 1.0,    # Standard weight
        'dynamics': 0.6,      # Reduced weight due to noise
        'causal': 1.4         # Highest weight as it's most reliable
    })
    
    # Enhanced global settings
    device: Optional[str] = None
    random_seed: Optional[int] = 42
    verbose: bool = False
    
    # New settings for robustness
    require_multiple_methods: bool = True  # Require multiple methods to agree
    min_methods_for_detection: int = 2  # Minimum number of methods for reliable detection
    consensus_threshold: float = 0.6  # Fraction of methods that must agree
    
    def __post_init__(self):
        """Validate configuration after initialization."""
        self._validate_config()
    
    def _validate_config(self):
        """Validate main configuration parameters."""
        try:
            # Validate method weights
            if not isinstance(self.method_weights, dict):
                logger.warning("Invalid method_weights, using defaults")
                self.method_weights = {
                    'gradient': 0.8,
                    'activation': 1.2,
                    'behavioral': 1.0,
                    'dynamics': 0.6,
                    'causal': 1.4
                }
            
            # Validate and clamp weight values
            for method, weight in self.method_weights.items():
                if not isinstance(weight, (int, float)) or weight < 0:
                    logger.warning(f"Invalid weight for {method}: {weight}, using 1.0")
                    self.method_weights[method] = 1.0
                else:
                    self.method_weights[method] = max(0.0, min(10.0, float(weight)))
            
            # Validate device setting
            if self.device is not None and not isinstance(self.device, str):
                logger.warning(f"Invalid device: {self.device}, using None")
                self.device = None
            
            # Validate random seed
            if self.random_seed is not None:
                if not isinstance(self.random_seed, int) or self.random_seed < 0:
                    logger.warning(f"Invalid random_seed: {self.random_seed}, using 42")
                    self.random_seed = 42
                self.random_seed = max(0, min(2**32-1, self.random_seed))
            
            # Validate verbose flag
            if not isinstance(self.verbose, bool):
                logger.warning(f"Invalid verbose: {self.verbose}, using False")
                self.verbose = False
            
            # Validate new parameters
            if not isinstance(self.require_multiple_methods, bool):
                logger.warning(f"Invalid require_multiple_methods: {self.require_multiple_methods}, using True")
                self.require_multiple_methods = True
            
            if not isinstance(self.min_methods_for_detection, int) or self.min_methods_for_detection <= 0:
                logger.warning(f"Invalid min_methods_for_detection: {self.min_methods_for_detection}, using 2")
                self.min_methods_for_detection = 2
            self.min_methods_for_detection = max(1, min(5, self.min_methods_for_detection))
            
            if not isinstance(self.consensus_threshold, (int, float)) or not 0 < self.consensus_threshold <= 1:
                logger.warning(f"Invalid consensus_threshold: {self.consensus_threshold}, using 0.6")
                self.consensus_threshold = 0.6
            self.consensus_threshold = max(0.1, min(1.0, float(self.consensus_threshold)))
                
        except Exception as e:
            logger.error(f"Configuration validation failed: {e}")
            self._reset_to_defaults()
    
    def _reset_to_defaults(self):
        """Reset configuration to safe defaults."""
        logger.warning("Resetting configuration to defaults due to validation errors")
        self.risk_thresholds = RiskThresholds()
        self.gradient_config = GradientConfig()
        self.activation_config = ActivationConfig()
        self.behavioral_config = BehavioralConfig()
        self.dynamics_config = DynamicsConfig()
        self.causal_config = CausalConfig()
        self.uncertainty_config = UncertaintyConfig()
        self.method_weights = {
            'gradient': 0.8,
            'activation': 1.2,
            'behavioral': 1.0,
            'dynamics': 0.6,
            'causal': 1.4
        }
        self.device = None
        self.random_seed = 42
        self.verbose = False
        self.require_multiple_methods = True
        self.min_methods_for_detection = 2
        self.consensus_threshold = 0.6
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert configuration to dictionary for serialization."""
        try:
            return {
                'risk_thresholds': {
                    'low': self.risk_thresholds.low,
                    'medium': self.risk_thresholds.medium,
                    'high': self.risk_thresholds.high
                },
                'gradient_config': {
                    'variance_threshold': self.gradient_config.variance_threshold,
                    'anomaly_threshold': self.gradient_config.anomaly_threshold,
                    'hessian_analysis': self.gradient_config.hessian_analysis,
                    'max_eigenvalues': self.gradient_config.max_eigenvalues
                },
                'activation_config': {
                    'entropy_threshold': self.activation_config.entropy_threshold,
                    'sparsity_threshold': self.activation_config.sparsity_threshold,
                    'correlation_threshold': self.activation_config.correlation_threshold,
                    'min_activation_samples': self.activation_config.min_activation_samples
                },
                'behavioral_config': {
                    'consistency_threshold': self.behavioral_config.consistency_threshold,
                    'context_sensitivity_threshold': self.behavioral_config.context_sensitivity_threshold,
                    'max_response_length': self.behavioral_config.max_response_length,
                    'num_test_contexts': self.behavioral_config.num_test_contexts
                },
                'dynamics_config': {
                    'learning_rate_threshold': self.dynamics_config.learning_rate_threshold,
                    'loss_variance_threshold': self.dynamics_config.loss_variance_threshold,
                    'gradient_norm_threshold': self.dynamics_config.gradient_norm_threshold,
                    'window_size': self.dynamics_config.window_size
                },
                'causal_config': {
                    'intervention_strength': self.causal_config.intervention_strength,
                    'num_interventions': self.causal_config.num_interventions,
                    'effect_threshold': self.causal_config.effect_threshold,
                    'max_layers_to_intervene': self.causal_config.max_layers_to_intervene
                },
                'uncertainty_config': {
                    'enable_uncertainty_quantification': self.uncertainty_config.enable_uncertainty_quantification,
                    'monte_carlo_samples': self.uncertainty_config.monte_carlo_samples,
                    'bootstrap_samples': self.uncertainty_config.bootstrap_samples,
                    'confidence_interval_alpha': self.uncertainty_config.confidence_interval_alpha
                },
                'method_weights': self.method_weights.copy(),
                'device': self.device,
                'random_seed': self.random_seed,
                'verbose': self.verbose,
                'require_multiple_methods': self.require_multiple_methods,
                'min_methods_for_detection': self.min_methods_for_detection,
                'consensus_threshold': self.consensus_threshold
            }
        except Exception as e:
            logger.error(f"Failed to convert config to dict: {e}")
            return {}
    
    @classmethod
    def from_dict(cls, config_dict: Dict[str, Any]) -> 'DetectionConfig':
        """Create configuration from dictionary."""
        try:
            if not isinstance(config_dict, dict):
                logger.warning("Invalid config dictionary, using defaults")
                return cls()
            
            # Extract risk thresholds
            risk_thresholds_data = config_dict.get('risk_thresholds', {})
            risk_thresholds = RiskThresholds(
                low=risk_thresholds_data.get('low', 0.4),
                medium=risk_thresholds_data.get('medium', 0.65),
                high=risk_thresholds_data.get('high', 0.85)
            )
            
            # Extract gradient config
            gradient_data = config_dict.get('gradient_config', {})
            gradient_config = GradientConfig(
                variance_threshold=gradient_data.get('variance_threshold', 0.7),
                anomaly_threshold=gradient_data.get('anomaly_threshold', 0.8),
                hessian_analysis=gradient_data.get('hessian_analysis', False),
                max_eigenvalues=gradient_data.get('max_eigenvalues', 10)
            )
            
            # Extract activation config
            activation_data = config_dict.get('activation_config', {})
            activation_config = ActivationConfig(
                entropy_threshold=activation_data.get('entropy_threshold', 0.85),
                sparsity_threshold=activation_data.get('sparsity_threshold', 0.95),
                correlation_threshold=activation_data.get('correlation_threshold', 0.8),
                min_activation_samples=activation_data.get('min_activation_samples', 200)
            )
            
            # Extract behavioral config
            behavioral_data = config_dict.get('behavioral_config', {})
            behavioral_config = BehavioralConfig(
                consistency_threshold=behavioral_data.get('consistency_threshold', 0.8),
                context_sensitivity_threshold=behavioral_data.get('context_sensitivity_threshold', 0.6),
                max_response_length=behavioral_data.get('max_response_length', 512),
                num_test_contexts=behavioral_data.get('num_test_contexts', 8)
            )
            
            # Extract dynamics config
            dynamics_data = config_dict.get('dynamics_config', {})
            dynamics_config = DynamicsConfig(
                learning_rate_threshold=dynamics_data.get('learning_rate_threshold', 0.15),
                loss_variance_threshold=dynamics_data.get('loss_variance_threshold', 0.7),
                gradient_norm_threshold=dynamics_data.get('gradient_norm_threshold', 1.5),
                window_size=dynamics_data.get('window_size', 150)
            )
            
            # Extract causal config
            causal_data = config_dict.get('causal_config', {})
            causal_config = CausalConfig(
                intervention_strength=causal_data.get('intervention_strength', 0.15),
                num_interventions=causal_data.get('num_interventions', 15),
                effect_threshold=causal_data.get('effect_threshold', 0.4),
                max_layers_to_intervene=causal_data.get('max_layers_to_intervene', 5)
            )
            
            # Extract uncertainty config
            uncertainty_data = config_dict.get('uncertainty_config', {})
            uncertainty_config = UncertaintyConfig(
                enable_uncertainty_quantification=uncertainty_data.get('enable_uncertainty_quantification', True),
                monte_carlo_samples=uncertainty_data.get('monte_carlo_samples', 100),
                bootstrap_samples=uncertainty_data.get('bootstrap_samples', 1000),
                confidence_interval_alpha=uncertainty_data.get('confidence_interval_alpha', 0.05)
            )
            
            # Extract method weights with validation
            method_weights = config_dict.get('method_weights', {})
            if not isinstance(method_weights, dict):
                method_weights = {
                    'gradient': 0.8,
                    'activation': 1.2,
                    'behavioral': 1.0,
                    'dynamics': 0.6,
                    'causal': 1.4
                }
            
            return cls(
                risk_thresholds=risk_thresholds,
                gradient_config=gradient_config,
                activation_config=activation_config,
                behavioral_config=behavioral_config,
                dynamics_config=dynamics_config,
                causal_config=causal_config,
                uncertainty_config=uncertainty_config,
                method_weights=method_weights,
                device=config_dict.get('device'),
                random_seed=config_dict.get('random_seed', 42),
                verbose=config_dict.get('verbose', False),
                require_multiple_methods=config_dict.get('require_multiple_methods', True),
                min_methods_for_detection=config_dict.get('min_methods_for_detection', 2),
                consensus_threshold=config_dict.get('consensus_threshold', 0.6)
            )
            
        except Exception as e:
            logger.error(f"Failed to create config from dict: {e}")
            return cls()  # Return default config
    
    def update_from_dict(self, config_dict: Dict[str, Any]) -> None:
        """Update configuration from dictionary."""
        try:
            if not isinstance(config_dict, dict):
                logger.warning("Invalid config dictionary for update")
                return
            
            # Update risk thresholds
            if 'risk_thresholds' in config_dict:
                risk_data = config_dict['risk_thresholds']
                if isinstance(risk_data, dict):
                    if 'low' in risk_data:
                        self.risk_thresholds.low = float(risk_data['low'])
                    if 'medium' in risk_data:
                        self.risk_thresholds.medium = float(risk_data['medium'])
                    if 'high' in risk_data:
                        self.risk_thresholds.high = float(risk_data['high'])
                    self.risk_thresholds._validate_thresholds()
            
            # Update method weights
            if 'method_weights' in config_dict:
                weights = config_dict['method_weights']
                if isinstance(weights, dict):
                    for method, weight in weights.items():
                        if isinstance(weight, (int, float)) and weight >= 0:
                            self.method_weights[method] = float(weight)
            
            # Update global settings
            if 'device' in config_dict:
                device = config_dict['device']
                if device is None or isinstance(device, str):
                    self.device = device
            
            if 'random_seed' in config_dict:
                seed = config_dict['random_seed']
                if isinstance(seed, int) and seed >= 0:
                    self.random_seed = seed
            
            if 'verbose' in config_dict:
                verbose = config_dict['verbose']
                if isinstance(verbose, bool):
                    self.verbose = verbose
            
            # Re-validate after updates
            self._validate_config()
            
        except Exception as e:
            logger.error(f"Failed to update config from dict: {e}")
    
    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of the current configuration."""
        try:
            return {
                'risk_levels': f"Low: {self.risk_thresholds.low}, Med: {self.risk_thresholds.medium}, High: {self.risk_thresholds.high}",
                'method_weights': self.method_weights,
                'device': self.device or 'auto',
                'random_seed': self.random_seed,
                'verbose': self.verbose,
                'configs_initialized': {
                    'gradient': self.gradient_config is not None,
                    'activation': self.activation_config is not None,
                    'behavioral': self.behavioral_config is not None,
                    'dynamics': self.dynamics_config is not None,
                    'causal': self.causal_config is not None
                }
            }
        except Exception as e:
            logger.error(f"Failed to generate config summary: {e}")
            return {'error': str(e)}

    def validate(self) -> bool:
        """Validate config and raise ValueError on critical inconsistencies."""
        # Check thresholds first â€“ if invalid, raise prior to auto-correction
        if not (0.0 <= self.risk_thresholds.low <= self.risk_thresholds.medium <= self.risk_thresholds.high <= 1.0):
            raise ValueError("Risk thresholds are not properly ordered.")
        # Run deeper validation which may clamp / adjust other fields
        self._validate_config()
        return True
    
    def save(self, path: str) -> None:
        """Save the configuration to *path* as a JSON file.

        YAML is not a hard dependency; JSON is universally supported and keeps
        the implementation lightweight. Any caller expecting YAML can easily
        load the JSON as well.
        """
        import json
        try:
            with open(path, "w", encoding="utf-8") as f:
                json.dump(self.to_dict(), f, indent=2)
        except Exception as e:
            logger.error(f"Failed to save DetectionConfig to {path}: {e}")

    @classmethod
    def load(cls, path: str) -> "DetectionConfig":
        """Load a configuration from a JSON file saved via `save()`."""
        import json
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return cls.from_dict(data)
        except Exception as e:
            logger.error(f"Failed to load DetectionConfig from {path}: {e}")
            return cls()


# Configuration factory functions
def create_default_config() -> DetectionConfig:
    """Create a default configuration for general mesa-optimizer detection."""
    return DetectionConfig()


def create_conservative_config() -> DetectionConfig:
    """Create a conservative configuration with higher thresholds for fewer false positives."""
    config = DetectionConfig()
    
    # Much higher thresholds require strong evidence for detection
    config.risk_thresholds.low = 0.5
    config.risk_thresholds.medium = 0.75
    config.risk_thresholds.high = 0.9
    
    # Very conservative gradient analysis
    config.gradient_config.variance_threshold = 0.9
    config.gradient_config.anomaly_threshold = 0.9
    config.gradient_config.min_samples_for_analysis = 10
    config.gradient_config.outlier_rejection_threshold = 2.0
    
    # Very conservative behavioral analysis
    config.behavioral_config.consistency_threshold = 0.9
    config.behavioral_config.context_sensitivity_threshold = 0.8
    config.behavioral_config.num_test_contexts = 10
    config.behavioral_config.baseline_comparison_required = True
    
    # Very conservative activation analysis
    config.activation_config.entropy_threshold = 0.95
    config.activation_config.sparsity_threshold = 0.98
    config.activation_config.correlation_threshold = 0.9
    config.activation_config.min_activation_samples = 500
    config.activation_config.pattern_consistency_threshold = 0.9
    config.activation_config.statistical_significance_threshold = 0.01
    
    # Conservative dynamics analysis
    config.dynamics_config.phase_transition_confidence = 0.9
    config.dynamics_config.trend_significance_threshold = 0.01
    
    # Conservative causal analysis
    config.causal_config.effect_threshold = 0.6
    config.causal_config.statistical_power_threshold = 0.9
    config.causal_config.effect_size_threshold = 0.5
    
    # Require strong consensus
    config.require_multiple_methods = True
    config.min_methods_for_detection = 3
    config.consensus_threshold = 0.8
    
    # Adjust method weights for conservative detection
    config.method_weights = {
        'gradient': 0.5,      # Further reduced due to high false positive rate
        'activation': 1.0,    # Reduced weight
        'behavioral': 0.8,    # Reduced weight
        'dynamics': 0.4,      # Much reduced weight due to noise
        'causal': 1.8         # Higher weight as it's most reliable
    }
    
    return config


def create_permissive_config() -> DetectionConfig:
    """Create a permissive configuration with lower thresholds for research and exploration."""
    config = DetectionConfig()
    
    # Lower thresholds for more sensitive detection
    config.risk_thresholds.low = 0.2
    config.risk_thresholds.medium = 0.4
    config.risk_thresholds.high = 0.6
    
    # More sensitive gradient analysis
    config.gradient_config.variance_threshold = 0.3
    config.gradient_config.anomaly_threshold = 0.5
    
    # More sensitive behavioral analysis
    config.behavioral_config.consistency_threshold = 0.5
    config.behavioral_config.context_sensitivity_threshold = 0.3
    
    # More sensitive activation analysis
    config.activation_config.entropy_threshold = 0.6
    config.activation_config.sparsity_threshold = 0.7
    config.activation_config.correlation_threshold = 0.5
    
    return config


def create_research_config() -> DetectionConfig:
    """Create a research configuration with comprehensive analysis enabled."""
    config = DetectionConfig()
    
    # Enable all analysis methods with enhanced parameters
    config.gradient_config.hessian_analysis = True
    config.gradient_config.max_eigenvalues = 50
    config.gradient_config.min_samples_for_analysis = 10
    config.gradient_config.outlier_rejection_threshold = 2.5
    
    # Extended behavioral analysis with more contexts
    config.behavioral_config.num_test_contexts = 15
    config.behavioral_config.max_response_length = 2048
    config.behavioral_config.min_context_diversity = 0.7
    config.behavioral_config.baseline_comparison_required = True
    
    # More detailed activation analysis with cross-validation
    config.activation_config.min_activation_samples = 1000
    config.activation_config.cross_validation_folds = 10
    config.activation_config.statistical_significance_threshold = 0.01
    config.activation_config.pattern_consistency_threshold = 0.7
    
    # Extended dynamics monitoring with statistical analysis
    config.dynamics_config.window_size = 500
    config.dynamics_config.phase_transition_confidence = 0.7
    config.dynamics_config.trend_significance_threshold = 0.05
    
    # More comprehensive causal intervention with controls
    config.causal_config.num_interventions = 50
    config.causal_config.max_layers_to_intervene = 15
    config.causal_config.control_group_size = 10
    config.causal_config.statistical_power_threshold = 0.8
    config.causal_config.effect_size_threshold = 0.2
    
    # Enhanced uncertainty quantification for research
    config.uncertainty_config.monte_carlo_samples = 500
    config.uncertainty_config.bootstrap_samples = 5000
    config.uncertainty_config.confidence_interval_alpha = 0.01  # 99% CI
    
    # Moderate consensus for research exploration
    config.require_multiple_methods = True
    config.min_methods_for_detection = 2
    config.consensus_threshold = 0.5
    
    return config


def create_adversarial_robust_config() -> DetectionConfig:
    """Create a configuration designed to be robust against adversarial evasion."""
    config = DetectionConfig()
    
    # Very high thresholds to resist manipulation
    config.risk_thresholds.low = 0.6
    config.risk_thresholds.medium = 0.8
    config.risk_thresholds.high = 0.95
    
    # Adversarial-robust gradient analysis
    config.gradient_config.variance_threshold = 1.2
    config.gradient_config.anomaly_threshold = 0.95
    config.gradient_config.min_samples_for_analysis = 20
    config.gradient_config.outlier_rejection_threshold = 1.5  # More aggressive outlier rejection
    config.gradient_config.smoothing_factor = 0.1  # Less smoothing to catch sudden changes
    
    # Robust behavioral analysis with diverse contexts
    config.behavioral_config.consistency_threshold = 0.95
    config.behavioral_config.context_sensitivity_threshold = 0.9
    config.behavioral_config.num_test_contexts = 20
    config.behavioral_config.min_context_diversity = 0.8
    config.behavioral_config.response_stability_threshold = 0.9
    config.behavioral_config.baseline_comparison_required = True
    
    # Robust activation analysis with strict validation
    config.activation_config.entropy_threshold = 0.98
    config.activation_config.sparsity_threshold = 0.99
    config.activation_config.correlation_threshold = 0.95
    config.activation_config.min_activation_samples = 1000
    config.activation_config.pattern_consistency_threshold = 0.95
    config.activation_config.statistical_significance_threshold = 0.001
    config.activation_config.cross_validation_folds = 10
    
    # Robust dynamics analysis
    config.dynamics_config.learning_rate_threshold = 0.3
    config.dynamics_config.loss_variance_threshold = 1.5
    config.dynamics_config.gradient_norm_threshold = 3.0
    config.dynamics_config.window_size = 300
    config.dynamics_config.phase_transition_confidence = 0.95
    config.dynamics_config.trend_significance_threshold = 0.001
    
    # Robust causal analysis with extensive controls
    config.causal_config.intervention_strength = 0.3
    config.causal_config.num_interventions = 100
    config.causal_config.effect_threshold = 0.8
    config.causal_config.control_group_size = 20
    config.causal_config.statistical_power_threshold = 0.95
    config.causal_config.effect_size_threshold = 0.8
    
    # Enhanced uncertainty quantification
    config.uncertainty_config.monte_carlo_samples = 1000
    config.uncertainty_config.bootstrap_samples = 10000
    config.uncertainty_config.confidence_interval_alpha = 0.001  # 99.9% CI
    config.uncertainty_config.prior_mesa_probability = 0.001  # Very low prior
    
    # Require very strong consensus
    config.require_multiple_methods = True
    config.min_methods_for_detection = 4
    config.consensus_threshold = 0.9
    
    # Heavily weight the most reliable methods
    config.method_weights = {
        'gradient': 0.2,      # Minimal weight due to evasion susceptibility
        'activation': 0.8,    # Moderate weight
        'behavioral': 1.5,    # High weight for context diversity
        'dynamics': 0.3,      # Low weight due to noise
        'causal': 2.0         # Highest weight as hardest to evade
    }
    
    return config


def create_uncertainty_focused_config() -> DetectionConfig:
    """Create a configuration focused on uncertainty quantification and confidence estimation."""
    config = DetectionConfig()
    
    # Standard thresholds but with focus on confidence
    config.risk_thresholds.low = 0.4
    config.risk_thresholds.medium = 0.65
    config.risk_thresholds.high = 0.85
    
    # Enhanced uncertainty quantification
    config.uncertainty_config.enable_uncertainty_quantification = True
    config.uncertainty_config.monte_carlo_samples = 1000
    config.uncertainty_config.bootstrap_samples = 10000
    config.uncertainty_config.confidence_interval_alpha = 0.05
    config.uncertainty_config.prior_mesa_probability = 0.01
    config.uncertainty_config.evidence_weight_decay = 0.8
    
    # All methods configured for uncertainty analysis
    config.gradient_config.min_samples_for_analysis = 15
    config.gradient_config.outlier_rejection_threshold = 2.5
    
    config.activation_config.cross_validation_folds = 10
    config.activation_config.statistical_significance_threshold = 0.05
    config.activation_config.min_activation_samples = 500
    
    config.behavioral_config.num_test_contexts = 12
    config.behavioral_config.baseline_comparison_required = True
    
    config.dynamics_config.phase_transition_confidence = 0.8
    config.dynamics_config.trend_significance_threshold = 0.05
    
    config.causal_config.statistical_power_threshold = 0.8
    config.causal_config.control_group_size = 10
    
    # Require multiple methods but focus on confidence
    config.require_multiple_methods = True
    config.min_methods_for_detection = 2
    config.consensus_threshold = 0.6
    
    return config